"""
PTF Tahmin Projesi - Model EÄŸitim ModÃ¼lÃ¼
========================================
XGBoost ve LightGBM ile PTF tahmin modelleri.

Ã–zellikler:
- Zaman serisi iÃ§in uygun train/test split
- Hiperparametre optimizasyonu (Optuna)
- Cross-validation
- SHAP analizi
- Model kaydetme/yÃ¼kleme
"""

import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import pickle
import json
import warnings
import logging

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ML KÃ¼tÃ¼phaneleri
try:
    import xgboost as xgb
    HAS_XGB = True
except ImportError:
    HAS_XGB = False
    logger.warning("XGBoost yÃ¼klÃ¼ deÄŸil: pip install xgboost")

try:
    import lightgbm as lgb
    HAS_LGB = True
except ImportError:
    HAS_LGB = False
    logger.warning("LightGBM yÃ¼klÃ¼ deÄŸil: pip install lightgbm")

from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler


# ============================================================================
# METRÄ°K FONKSÄ°YONLARI
# ============================================================================

def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Mean Absolute Percentage Error (MAPE).
    
    NOT: Ã‡ok dÃ¼ÅŸÃ¼k gerÃ§ek deÄŸerler (< 50 TL) MAPE'yi patlatÄ±r.
    Bu deÄŸerler filtrelenir Ã§Ã¼nkÃ¼:
    1. PTF nadiren 50 TL'nin altÄ±na dÃ¼ÅŸer
    2. DÃ¼ÅŸÃ¼k deÄŸerler genellikle hatalÄ± veri veya Ã¶zel durumlar
    3. Ticari aÃ§Ä±dan yÃ¼ksek fiyatlÄ± saatler daha Ã¶nemli
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    
    # Ã‡ok dÃ¼ÅŸÃ¼k deÄŸerleri filtrele (< 50 TL)
    mask = y_true > 50
    
    if mask.sum() == 0:
        # HiÃ§ deÄŸer kalmadÄ±ysa eski yÃ¶ntemle hesapla
        mask = y_true != 0
    
    if mask.sum() == 0:
        return np.nan
    
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100


def smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Symmetric Mean Absolute Percentage Error (SMAPE).
    
    MAPE'den daha dengeli - aÅŸÄ±rÄ± dÃ¼ÅŸÃ¼k/yÃ¼ksek deÄŸerlere karÅŸÄ± dayanÄ±klÄ±.
    Range: 0-200%
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
    
    # Ã‡ok kÃ¼Ã§Ã¼k payda deÄŸerlerini filtrele
    mask = denominator > 10
    
    if mask.sum() == 0:
        mask = denominator != 0
    
    if mask.sum() == 0:
        return np.nan
    
    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / denominator[mask]) * 100


def weighted_mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Hacim AÄŸÄ±rlÄ±klÄ± MAPE - YÃ¼ksek fiyatlÄ± saatlere daha fazla aÄŸÄ±rlÄ±k verir.
    
    Enerji piyasasÄ±nda yÃ¼ksek fiyatlÄ± saatler ticari aÃ§Ä±dan daha Ã¶nemlidir.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    
    # Ã‡ok dÃ¼ÅŸÃ¼k deÄŸerleri filtrele
    mask = y_true > 50
    
    if mask.sum() == 0:
        return np.nan
    
    y_true_f = y_true[mask]
    y_pred_f = y_pred[mask]
    
    # AÄŸÄ±rlÄ±klar = gerÃ§ek deÄŸerler (yÃ¼ksek fiyat = yÃ¼ksek aÄŸÄ±rlÄ±k)
    weights = y_true_f
    errors = np.abs((y_true_f - y_pred_f) / y_true_f)
    
    return np.average(errors, weights=weights) * 100


def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """TÃ¼m metrikleri hesaplar"""
    return {
        'mae': mean_absolute_error(y_true, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
        'mape': mape(y_true, y_pred),
        'smape': smape(y_true, y_pred),
        'weighted_mape': weighted_mape(y_true, y_pred),
        'r2': r2_score(y_true, y_pred)
    }


# ============================================================================
# VERÄ° HAZIRLAMA
# ============================================================================

class DataPreparer:
    """
    Model eÄŸitimi iÃ§in veri hazÄ±rlama sÄ±nÄ±fÄ±.
    
    - Train/Test/Validation split (zaman serisi uyumlu)
    - Eksik veri temizleme
    - Ã–znitelik seÃ§imi
    """
    
    def __init__(
        self, 
        target_col: str = 'target_ptf_72h',
        test_size: float = 0.2,
        val_size: float = 0.1
    ):
        self.target_col = target_col
        self.test_size = test_size
        self.val_size = val_size
        self.feature_cols = None
        self.scaler = None
    
    def prepare(
        self, 
        df: pd.DataFrame,
        drop_cols: List[str] = None,
        scale_features: bool = False
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Veriyi train/val/test olarak ayÄ±rÄ±r.
        
        Ã–NEMLI: Zaman serisi iÃ§in random split YAPILMAZ!
        Kronolojik sÄ±ra korunur.
        
        Returns:
            Tuple[train_df, val_df, test_df]
        """
        df = df.copy()
        
        # Hedef deÄŸiÅŸken kontrolÃ¼
        if self.target_col not in df.columns:
            raise ValueError(f"Hedef kolon bulunamadÄ±: {self.target_col}")
        
        # KaldÄ±rÄ±lacak kolonlar
        drop_cols = drop_cols or []
        target_cols = [c for c in df.columns if 'target' in c and c != self.target_col]
        
        # Ham fiyat kolonlarÄ±nÄ± kaldÄ±r (data leakage Ã¶nleme)
        leaky_cols = ['ptf', 'smf']
        
        # DATETIME KOLONLARINI KALDIR (XGBoost bunlarÄ± iÅŸleyemez)
        datetime_cols = []
        for col in df.columns:
            if df[col].dtype.name.startswith('datetime') or 'datetime' in str(df[col].dtype):
                datetime_cols.append(col)
            elif df[col].dtype == 'object':
                # Object tipindeki kolonlarÄ± kontrol et
                try:
                    pd.to_datetime(df[col].iloc[0])
                    datetime_cols.append(col)
                except:
                    pass
        
        if datetime_cols:
            logger.info(f"Datetime kolonlarÄ± kaldÄ±rÄ±lÄ±yor: {datetime_cols}")
        
        cols_to_drop = list(set(drop_cols + target_cols + leaky_cols + datetime_cols))
        
        # Ã–znitelik kolonlarÄ±nÄ± belirle
        self.feature_cols = [
            c for c in df.columns 
            if c not in cols_to_drop and c != self.target_col
        ]
        
        # SayÄ±sal olmayan kolonlarÄ± da kaldÄ±r
        non_numeric_cols = []
        for col in self.feature_cols:
            if df[col].dtype not in ['float64', 'float32', 'int64', 'int32', 'bool', 'int8', 'int16', 'float16']:
                non_numeric_cols.append(col)
        
        if non_numeric_cols:
            logger.warning(f"SayÄ±sal olmayan kolonlar kaldÄ±rÄ±lÄ±yor: {non_numeric_cols}")
            self.feature_cols = [c for c in self.feature_cols if c not in non_numeric_cols]
        
        logger.info(f"Toplam {len(self.feature_cols)} Ã¶znitelik kullanÄ±lacak")
        
        # Eksik deÄŸerleri temizle - AMA Ã¶nce kaÃ§ satÄ±r kaybedeceÄŸimizi kontrol et
        df_subset = df[[self.target_col] + self.feature_cols]
        
        # Her kolondaki eksik veri yÃ¼zdesini kontrol et
        missing_pct = (df_subset.isnull().sum() / len(df_subset) * 100).round(2)
        high_missing = missing_pct[missing_pct > 50]
        
        if not high_missing.empty:
            logger.warning(f"YÃ¼ksek eksik verili kolonlar kaldÄ±rÄ±lÄ±yor (>50%): {list(high_missing.index)}")
            cols_to_remove = list(high_missing.index)
            self.feature_cols = [c for c in self.feature_cols if c not in cols_to_remove]
            df_subset = df[[self.target_col] + self.feature_cols]
        
        # Åimdi eksik satÄ±rlarÄ± temizle
        initial_rows = len(df_subset)
        
        # INF DEÄERLERÄ° TEMÄ°ZLE (XGBoost inf kabul etmiyor!)
        df_subset = df_subset.replace([np.inf, -np.inf], np.nan)
        
        df_clean = df_subset.dropna()
        final_rows = len(df_clean)
        
        logger.info(f"Eksik veri temizlendi: {initial_rows} â†’ {final_rows} satÄ±r ({initial_rows - final_rows} satÄ±r Ã§Ä±karÄ±ldÄ±)")
        
        if final_rows == 0:
            # EÄŸer tÃ¼m satÄ±rlar silindiyse, forward fill dene
            logger.warning("TÃ¼m satÄ±rlar silindi! Forward fill deneniyor...")
            df_subset = df_subset.fillna(method='ffill').fillna(method='bfill')
            df_clean = df_subset.dropna()
            final_rows = len(df_clean)
            logger.info(f"Forward fill sonrasÄ±: {final_rows} satÄ±r")
        
        if final_rows == 0:
            raise ValueError("Veri hazÄ±rlama sonrasÄ± 0 satÄ±r kaldÄ±! Veri kalitesini kontrol edin.")
        
        # Kronolojik split
        n = len(df_clean)
        test_idx = int(n * (1 - self.test_size))
        val_idx = int(test_idx * (1 - self.val_size))
        
        train_df = df_clean.iloc[:val_idx]
        val_df = df_clean.iloc[val_idx:test_idx]
        test_df = df_clean.iloc[test_idx:]
        
        logger.info(f"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")
        logger.info(f"Train period: {train_df.index.min()} â†’ {train_df.index.max()}")
        logger.info(f"Test period: {test_df.index.min()} â†’ {test_df.index.max()}")
        
        # Ã–lÃ§ekleme (opsiyonel)
        if scale_features:
            self.scaler = StandardScaler()
            train_df[self.feature_cols] = self.scaler.fit_transform(train_df[self.feature_cols])
            val_df[self.feature_cols] = self.scaler.transform(val_df[self.feature_cols])
            test_df[self.feature_cols] = self.scaler.transform(test_df[self.feature_cols])
        
        return train_df, val_df, test_df
    
    def get_xy(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """DataFrame'den X ve y ayÄ±rÄ±r"""
        X = df[self.feature_cols]
        y = df[self.target_col]
        return X, y


# ============================================================================
# MODEL EÄÄ°TÄ°CÄ°
# ============================================================================

class PTFModelTrainer:
    """
    PTF tahmin modeli eÄŸitici.
    
    Desteklenen modeller:
    - XGBoost (varsayÄ±lan)
    - LightGBM
    """
    
    # VarsayÄ±lan hiperparametreler (PTF iÃ§in optimize edilmiÅŸ)
    DEFAULT_XGB_PARAMS = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'max_depth': 8,
        'learning_rate': 0.05,
        'n_estimators': 500,
        'min_child_weight': 5,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'random_state': 42,
        'n_jobs': -1,
        'verbosity': 0
    }
    
    DEFAULT_LGB_PARAMS = {
        'objective': 'regression',
        'metric': 'rmse',
        'max_depth': 8,
        'learning_rate': 0.05,
        'n_estimators': 500,
        'num_leaves': 64,
        'min_child_samples': 20,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'random_state': 42,
        'n_jobs': -1,
        'verbose': -1
    }
    
    def __init__(
        self, 
        model_type: str = 'xgboost',
        params: Dict = None
    ):
        """
        Args:
            model_type: 'xgboost' veya 'lightgbm'
            params: Model parametreleri (None ise varsayÄ±lan kullanÄ±lÄ±r)
        """
        self.model_type = model_type.lower()
        self.model = None
        self.feature_importance = None
        self.training_history = []
        
        # Parametreleri ayarla
        if self.model_type == 'xgboost':
            if not HAS_XGB:
                raise ImportError("XGBoost yÃ¼klÃ¼ deÄŸil!")
            self.params = params or self.DEFAULT_XGB_PARAMS.copy()
        elif self.model_type == 'lightgbm':
            if not HAS_LGB:
                raise ImportError("LightGBM yÃ¼klÃ¼ deÄŸil!")
            self.params = params or self.DEFAULT_LGB_PARAMS.copy()
        else:
            raise ValueError(f"Desteklenmeyen model: {model_type}")
    
    def train(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame = None,
        y_val: pd.Series = None,
        early_stopping_rounds: int = 50
    ) -> 'PTFModelTrainer':
        """
        Modeli eÄŸitir.
        
        Args:
            X_train: EÄŸitim Ã¶zellikleri
            y_train: EÄŸitim hedefi
            X_val: Validasyon Ã¶zellikleri (erken durdurma iÃ§in)
            y_val: Validasyon hedefi
            early_stopping_rounds: Erken durdurma sabrÄ±
            
        Returns:
            self (zincirleme iÃ§in)
        """
        logger.info(f"\n{'='*50}")
        logger.info(f"{self.model_type.upper()} MODEL EÄÄ°TÄ°MÄ°")
        logger.info(f"{'='*50}")
        logger.info(f"Train shape: {X_train.shape}")
        
        start_time = datetime.now()
        
        if self.model_type == 'xgboost':
            self._train_xgboost(X_train, y_train, X_val, y_val, early_stopping_rounds)
        else:
            self._train_lightgbm(X_train, y_train, X_val, y_val, early_stopping_rounds)
        
        training_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"EÄŸitim sÃ¼resi: {training_time:.1f} saniye")
        
        # Feature importance hesapla
        self._calculate_feature_importance(X_train.columns.tolist())
        
        return self
    
    def _train_xgboost(self, X_train, y_train, X_val, y_val, early_stopping_rounds):
        """XGBoost eÄŸitimi"""
        self.model = xgb.XGBRegressor(**self.params)
        
        eval_set = [(X_train, y_train)]
        if X_val is not None and y_val is not None:
            eval_set.append((X_val, y_val))
        
        self.model.fit(
            X_train, y_train,
            eval_set=eval_set,
            verbose=False
        )
        
        # En iyi iterasyon
        if hasattr(self.model, 'best_iteration'):
            logger.info(f"En iyi iterasyon: {self.model.best_iteration}")
    
    def _train_lightgbm(self, X_train, y_train, X_val, y_val, early_stopping_rounds):
        """LightGBM eÄŸitimi"""
        self.model = lgb.LGBMRegressor(**self.params)
        
        eval_set = [(X_train, y_train)]
        if X_val is not None and y_val is not None:
            eval_set.append((X_val, y_val))
        
        callbacks = [lgb.log_evaluation(period=0)]
        if early_stopping_rounds:
            callbacks.append(lgb.early_stopping(early_stopping_rounds, verbose=False))
        
        self.model.fit(
            X_train, y_train,
            eval_set=eval_set,
            callbacks=callbacks
        )
        
        if hasattr(self.model, 'best_iteration_'):
            logger.info(f"En iyi iterasyon: {self.model.best_iteration_}")
    
    def _calculate_feature_importance(self, feature_names: List[str]):
        """Ã–znitelik Ã¶nem skorlarÄ±nÄ± hesaplar"""
        if self.model is None:
            return
        
        importance = self.model.feature_importances_
        self.feature_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Tahmin yapar"""
        if self.model is None:
            raise ValueError("Model henÃ¼z eÄŸitilmedi!")
        return self.model.predict(X)
    
    def evaluate(
        self, 
        X_test: pd.DataFrame, 
        y_test: pd.Series,
        verbose: bool = True
    ) -> Dict[str, float]:
        """
        Model performansÄ±nÄ± deÄŸerlendirir.
        
        Returns:
            Dict: Metrik adÄ± â†’ deÄŸer
        """
        y_pred = self.predict(X_test)
        metrics = calculate_metrics(y_test.values, y_pred)
        
        if verbose:
            logger.info(f"\nğŸ“Š MODEL PERFORMANSI")
            logger.info(f"   MAE:   {metrics['mae']:.2f} TL")
            logger.info(f"   RMSE:  {metrics['rmse']:.2f} TL")
            logger.info(f"   MAPE:  {metrics['mape']:.2f}%")
            logger.info(f"   Weighted MAPE: {metrics.get('weighted_mape', 0):.2f}%")
            logger.info(f"   SMAPE: {metrics['smape']:.2f}%")
            logger.info(f"   RÂ²:    {metrics['r2']:.4f}")
        
        return metrics
    
    def cross_validate(
        self, 
        X: pd.DataFrame, 
        y: pd.Series,
        n_splits: int = 5
    ) -> Dict[str, List[float]]:
        """
        Zaman serisi cross-validation uygular.
        
        TimeSeriesSplit kullanÄ±r (gelecek verisi sÄ±zmaz).
        """
        logger.info(f"\nğŸ”„ {n_splits}-Fold Time Series Cross Validation")
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        cv_results = {metric: [] for metric in ['mae', 'rmse', 'mape', 'r2']}
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
            X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
            y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]
            
            # Yeni model oluÅŸtur ve eÄŸit
            cv_model = PTFModelTrainer(self.model_type, self.params.copy())
            cv_model.train(X_train_cv, y_train_cv)
            
            # DeÄŸerlendir
            metrics = cv_model.evaluate(X_val_cv, y_val_cv, verbose=False)
            
            for metric, value in metrics.items():
                if metric in cv_results:
                    cv_results[metric].append(value)
            
            logger.info(f"  Fold {fold}: MAPE={metrics['mape']:.2f}%, RÂ²={metrics['r2']:.4f}")
        
        # Ortalama sonuÃ§lar
        logger.info(f"\n  ğŸ“ˆ CV OrtalamalarÄ±:")
        for metric, values in cv_results.items():
            logger.info(f"     {metric.upper()}: {np.mean(values):.4f} (Â±{np.std(values):.4f})")
        
        return cv_results
    
    def get_top_features(self, n: int = 20) -> pd.DataFrame:
        """En Ã¶nemli n Ã¶zniteliÄŸi dÃ¶ndÃ¼rÃ¼r"""
        if self.feature_importance is None:
            return pd.DataFrame()
        return self.feature_importance.head(n)
    
    def save(self, path: str):
        """Modeli kaydeder"""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        save_dict = {
            'model': self.model,
            'model_type': self.model_type,
            'params': self.params,
            'feature_importance': self.feature_importance
        }
        
        with open(path, 'wb') as f:
            pickle.dump(save_dict, f)
        
        logger.info(f"âœ“ Model kaydedildi: {path}")
    
    @classmethod
    def load(cls, path: str) -> 'PTFModelTrainer':
        """Modeli yÃ¼kler"""
        with open(path, 'rb') as f:
            save_dict = pickle.load(f)
        
        trainer = cls(
            model_type=save_dict['model_type'],
            params=save_dict['params']
        )
        trainer.model = save_dict['model']
        trainer.feature_importance = save_dict['feature_importance']
        
        logger.info(f"âœ“ Model yÃ¼klendi: {path}")
        return trainer


# ============================================================================
# SHAP ANALÄ°ZÄ°
# ============================================================================

def analyze_with_shap(
    model: PTFModelTrainer,
    X: pd.DataFrame,
    max_samples: int = 1000
) -> Optional[Any]:
    """
    SHAP deÄŸerlerini hesaplar ve gÃ¶rselleÅŸtirir.
    
    SHAP, modelin her tahmin iÃ§in hangi Ã¶zelliklerin
    ne kadar etkili olduÄŸunu gÃ¶sterir.
    """
    try:
        import shap
    except ImportError:
        logger.warning("SHAP yÃ¼klÃ¼ deÄŸil: pip install shap")
        return None
    
    logger.info("\nğŸ” SHAP Analizi yapÄ±lÄ±yor...")
    
    # Ã–rnek sayÄ±sÄ±nÄ± sÄ±nÄ±rla (hesaplama sÃ¼resi iÃ§in)
    if len(X) > max_samples:
        X_sample = X.sample(max_samples, random_state=42)
    else:
        X_sample = X
    
    # SHAP explainer oluÅŸtur
    explainer = shap.TreeExplainer(model.model)
    shap_values = explainer.shap_values(X_sample)
    
    # Ã–zet istatistikler
    mean_abs_shap = np.abs(shap_values).mean(axis=0)
    shap_importance = pd.DataFrame({
        'feature': X.columns,
        'shap_importance': mean_abs_shap
    }).sort_values('shap_importance', ascending=False)
    
    logger.info("\nSHAP Ã–nem SÄ±ralamasÄ± (Top 10):")
    for i, row in shap_importance.head(10).iterrows():
        logger.info(f"  {row['feature']}: {row['shap_importance']:.2f}")
    
    return shap_values


# ============================================================================
# TEST
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("MODEL EÄÄ°TÄ°M TESTÄ°")
    print("="*60 + "\n")
    
    # Ã–rnek veri oluÅŸtur
    np.random.seed(42)
    n = 5000
    dates = pd.date_range('2022-01-01', periods=n, freq='h')
    
    # GerÃ§ekÃ§i PTF simÃ¼lasyonu
    base = 100
    trend = np.linspace(0, 50, n)  # YÃ¼kselen trend
    daily = 30 * np.sin(2 * np.pi * np.arange(n) / 24)  # GÃ¼nlÃ¼k dÃ¶ngÃ¼
    weekly = 15 * np.sin(2 * np.pi * np.arange(n) / 168)  # HaftalÄ±k dÃ¶ngÃ¼
    noise = np.random.randn(n) * 20
    
    ptf = base + trend + daily + weekly + noise
    ptf = np.maximum(ptf, 0)  # Negatif fiyat olamaz
    
    df = pd.DataFrame({'ptf': ptf}, index=dates)
    
    # Basit lag Ã¶zellikleri
    for lag in [24, 48, 168]:
        df[f'ptf_lag_{lag}'] = df['ptf'].shift(lag + 72)  # 72 saat sonrasÄ± iÃ§in
    
    df['hour'] = df.index.hour
    df['day_of_week'] = df.index.dayofweek
    df['month'] = df.index.month
    
    # Hedef deÄŸiÅŸken
    df['target_ptf_72h'] = df['ptf'].shift(-72)
    
    print(f"Veri shape: {df.shape}")
    
    # Veri hazÄ±rlama
    preparer = DataPreparer(target_col='target_ptf_72h')
    train_df, val_df, test_df = preparer.prepare(df, drop_cols=['ptf'])
    
    X_train, y_train = preparer.get_xy(train_df)
    X_val, y_val = preparer.get_xy(val_df)
    X_test, y_test = preparer.get_xy(test_df)
    
    # Model eÄŸitimi
    if HAS_XGB:
        trainer = PTFModelTrainer(model_type='xgboost')
        trainer.train(X_train, y_train, X_val, y_val)
        
        # DeÄŸerlendirme
        metrics = trainer.evaluate(X_test, y_test)
        
        # Top Ã¶zellikler
        print("\nğŸ“Š En Ã–nemli Ã–zellikler:")
        print(trainer.get_top_features(10))
        
        # Model kaydet
        trainer.save('/home/claude/ptf_project/models/test_model.pkl')
    else:
        print("XGBoost yÃ¼klÃ¼ deÄŸil, test atlandÄ±.")
