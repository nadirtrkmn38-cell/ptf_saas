"""
PTF Tahmin Projesi - Veri Ã‡ekme ModÃ¼lÃ¼
======================================
EPÄ°AÅ ÅeffaflÄ±k Platformu'ndan PTF tahminlemesi iÃ§in 
gerekli tÃ¼m verileri Ã§eker ve birleÅŸtirir.

Veri KaynaklarÄ±:
- PTF (Piyasa Takas FiyatÄ±) - mcp
- SMF (Sistem Marjinal FiyatÄ±) - smp  
- YÃ¼k Tahmini - load-plan
- RÃ¼zgar Tahmini - wind-forecast
- GerÃ§ek ZamanlÄ± Ãœretim - realtime-generation
- KGÃœP (GÃ¼nlÃ¼k Ãœretim PlanÄ±) - dpp-org
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, List, Tuple
import logging
import pickle
import warnings

warnings.filterwarnings('ignore')

# Proje path'ini ayarla
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from config.settings import get_settings, Settings

# Logging ayarlarÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EPIASDataFetcher:
    """
    EPÄ°AÅ ÅeffaflÄ±k Platformu'ndan veri Ã§eken sÄ±nÄ±f.
    
    Attributes:
        eptr: EPTR2 client instance
        settings: Proje ayarlarÄ±
        cache_dir: Cache dizini
    """
    
    def __init__(self, settings: Optional[Settings] = None):
        """
        Args:
            settings: Proje ayarlarÄ±. None ise config'den yÃ¼klenir.
        """
        self.settings = settings or get_settings()
        self.cache_dir = PROJECT_ROOT / self.settings.data.cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # EPTR2 client'Ä± baÅŸlat
        self._init_eptr_client()
    
    def _init_eptr_client(self):
        """EPTR2 client'Ä± kimlik bilgileriyle baÅŸlatÄ±r."""
        try:
            from eptr2 import EPTR2
            
            self.eptr = EPTR2(
                username=self.settings.epias.username,
                password=self.settings.epias.password
            )
            logger.info("âœ“ EPÄ°AÅ baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±")
            
        except Exception as e:
            logger.error(f"âœ— EPÄ°AÅ baÄŸlantÄ± hatasÄ±: {e}")
            raise ConnectionError(
                "EPÄ°AÅ'a baÄŸlanÄ±lamadÄ±. LÃ¼tfen config/config.yaml dosyasÄ±ndaki "
                "kullanÄ±cÄ± adÄ± ve ÅŸifreyi kontrol edin."
            )
    
    def _get_cache_path(self, data_type: str, start: str, end: str) -> Path:
        """Cache dosyasÄ± path'ini oluÅŸturur."""
        filename = f"{data_type}_{start}_{end}.pkl"
        return self.cache_dir / filename
    
    def _load_from_cache(self, cache_path: Path) -> Optional[pd.DataFrame]:
        """Cache'ten veri yÃ¼kler."""
        if self.settings.data.cache_enabled and cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    df = pickle.load(f)
                logger.info(f"  â†³ Cache'ten yÃ¼klendi: {cache_path.name}")
                return df
            except Exception as e:
                logger.warning(f"  â†³ Cache okuma hatasÄ±: {e}")
        return None
    
    def _save_to_cache(self, df: pd.DataFrame, cache_path: Path):
        """Veriyi cache'e kaydeder."""
        if self.settings.data.cache_enabled:
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump(df, f)
                logger.info(f"  â†³ Cache'e kaydedildi: {cache_path.name}")
            except Exception as e:
                logger.warning(f"  â†³ Cache yazma hatasÄ±: {e}")
    
    def _fetch_data(
        self, 
        call_name: str, 
        start_date: str, 
        end_date: str,
        use_cache: bool = True
    ) -> pd.DataFrame:
        """
        EPÄ°AÅ'tan veri Ã§eker.
        
        NOT: EPÄ°AÅ API maksimum 1 yÄ±llÄ±k veri Ã§ekmeye izin verir.
        Bu fonksiyon otomatik olarak tarihleri parÃ§alara bÃ¶ler.
        
        Args:
            call_name: API endpoint adÄ± (mcp, smp, load-plan vb.)
            start_date: BaÅŸlangÄ±Ã§ tarihi (YYYY-MM-DD)
            end_date: BitiÅŸ tarihi (YYYY-MM-DD)
            use_cache: Cache kullanÄ±lsÄ±n mÄ±?
            
        Returns:
            DataFrame: Ã‡ekilen veri
        """
        cache_path = self._get_cache_path(call_name, start_date, end_date)
        
        # Cache kontrolÃ¼
        if use_cache:
            cached_df = self._load_from_cache(cache_path)
            if cached_df is not None:
                return cached_df
        
        # Tarihleri parÃ§alara bÃ¶l (EPÄ°AÅ max 1 yÄ±l izin veriyor)
        date_chunks = self._split_date_range(start_date, end_date, max_days=364)
        
        all_data = []
        
        for chunk_start, chunk_end in date_chunks:
            logger.info(f"  â†’ {call_name} Ã§ekiliyor: {chunk_start} - {chunk_end}")
            
            try:
                df = self.eptr.call(
                    call_name,
                    start_date=chunk_start,
                    end_date=chunk_end
                )
                
                if df is not None and not df.empty:
                    all_data.append(df)
                    logger.info(f"    âœ“ {len(df)} satÄ±r Ã§ekildi")
                else:
                    logger.warning(f"    âš  BoÅŸ veri: {chunk_start} - {chunk_end}")
                    
            except Exception as e:
                logger.error(f"    âœ— Hata ({chunk_start} - {chunk_end}): {e}")
                continue
        
        # TÃ¼m parÃ§alarÄ± birleÅŸtir
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            combined_df = combined_df.drop_duplicates()
            self._save_to_cache(combined_df, cache_path)
            logger.info(f"  âœ“ Toplam {len(combined_df)} satÄ±r Ã§ekildi")
            return combined_df
        else:
            logger.warning(f"  âš  HiÃ§ veri Ã§ekilemedi: {call_name}")
            return pd.DataFrame()
    
    def _split_date_range(
        self, 
        start_date: str, 
        end_date: str, 
        max_days: int = 364
    ) -> list:
        """
        Tarih aralÄ±ÄŸÄ±nÄ± parÃ§alara bÃ¶ler.
        
        EPÄ°AÅ API maksimum 1 yÄ±l (365 gÃ¼n) veri Ã§ekmeye izin verir.
        GÃ¼venli olmak iÃ§in 364 gÃ¼n kullanÄ±yoruz.
        
        Returns:
            List of (start, end) tuples
        """
        from datetime import datetime, timedelta
        
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        
        chunks = []
        current_start = start
        
        while current_start < end:
            current_end = min(current_start + timedelta(days=max_days), end)
            chunks.append((
                current_start.strftime("%Y-%m-%d"),
                current_end.strftime("%Y-%m-%d")
            ))
            current_start = current_end + timedelta(days=1)
        
        logger.info(f"  ğŸ“… Tarih aralÄ±ÄŸÄ± {len(chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼")
        return chunks
    
    def fetch_ptf(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        PTF (Piyasa Takas FiyatÄ±) verilerini Ã§eker.
        
        Returns:
            DataFrame: datetime index, ptf kolonu
        """
        logger.info("ğŸ“Š PTF verileri Ã§ekiliyor...")
        df = self._fetch_data("mcp", start_date, end_date)
        
        if not df.empty:
            # Debug: kolonlarÄ± gÃ¶ster
            logger.info(f"  PTF kolonlarÄ±: {list(df.columns)}")
            
            # Kolon isimlerini standartlaÅŸtÄ±r
            df = self._standardize_datetime(df)
            
            # PTF kolonunu bul ve yeniden adlandÄ±r
            ptf_patterns = ['marketclearingprice', 'mcp', 'price', 'ptf', 'fiyat']
            ptf_col = None
            
            for col in df.columns:
                if col.lower() in ptf_patterns or any(p in col.lower() for p in ptf_patterns):
                    ptf_col = col
                    break
            
            if ptf_col and ptf_col != 'ptf':
                df = df.rename(columns={ptf_col: 'ptf'})
                logger.info(f"  '{ptf_col}' -> 'ptf' olarak yeniden adlandÄ±rÄ±ldÄ±")
            
            # PTF kolonunu sayÄ±sala Ã§evir
            if 'ptf' in df.columns:
                df['ptf'] = pd.to_numeric(df['ptf'], errors='coerce')
        
        return df
    
    def fetch_smf(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        SMF (Sistem Marjinal FiyatÄ±) verilerini Ã§eker.
        
        Returns:
            DataFrame: datetime index, smf kolonu
        """
        logger.info("ğŸ“Š SMF verileri Ã§ekiliyor...")
        df = self._fetch_data("smp", start_date, end_date)
        
        if not df.empty:
            logger.info(f"  SMF kolonlarÄ±: {list(df.columns)}")
            
            df = self._standardize_datetime(df)
            
            # SMF kolonunu bul
            smf_patterns = ['systemmarginalprice', 'smp', 'smf', 'price', 'fiyat']
            smf_col = None
            
            for col in df.columns:
                if col.lower() in smf_patterns or any(p in col.lower() for p in smf_patterns):
                    smf_col = col
                    break
            
            if smf_col and smf_col != 'smf':
                df = df.rename(columns={smf_col: 'smf'})
                logger.info(f"  '{smf_col}' -> 'smf' olarak yeniden adlandÄ±rÄ±ldÄ±")
            
            if 'smf' in df.columns:
                df['smf'] = pd.to_numeric(df['smf'], errors='coerce')
        
        return df
    
    def fetch_load_plan(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        YÃ¼k tahmin planÄ±nÄ± Ã§eker.
        
        Returns:
            DataFrame: datetime index, load_forecast kolonu
        """
        logger.info("ğŸ“Š YÃ¼k tahmini Ã§ekiliyor...")
        df = self._fetch_data("load-plan", start_date, end_date)
        
        if not df.empty:
            logger.info(f"  YÃ¼k kolonlarÄ±: {list(df.columns)}")
            
            df = self._standardize_datetime(df)
            
            # YÃ¼k kolonunu bul
            load_patterns = ['lep', 'loadestimationplan', 'load', 'forecast', 'demand', 'consumption', 'tuketim', 'talep']
            load_col = None
            
            for col in df.columns:
                if col.lower() in load_patterns or any(p in col.lower() for p in load_patterns):
                    load_col = col
                    break
            
            if load_col and load_col != 'load_forecast':
                df = df.rename(columns={load_col: 'load_forecast'})
                logger.info(f"  '{load_col}' -> 'load_forecast' olarak yeniden adlandÄ±rÄ±ldÄ±")
            
            if 'load_forecast' in df.columns:
                df['load_forecast'] = pd.to_numeric(df['load_forecast'], errors='coerce')
        
        return df
    
    def fetch_wind_forecast(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        RÃ¼zgar Ã¼retim tahminini Ã§eker.
        
        Returns:
            DataFrame: datetime index, wind_generation/forecast kolonlarÄ±
        """
        logger.info("ğŸ“Š RÃ¼zgar tahmini Ã§ekiliyor...")
        df = self._fetch_data("wind-forecast", start_date, end_date)
        
        if not df.empty:
            logger.info(f"  RÃ¼zgar kolonlarÄ±: {list(df.columns)}")
            
            df = self._standardize_datetime(df)
            
            # SayÄ±sal kolonlarÄ± dÃ¶nÃ¼ÅŸtÃ¼r
            numeric_patterns = ['forecast', 'generation', 'quarter', 'wind', 'actual', 'uretim']
            
            for col in df.columns:
                col_lower = col.lower()
                if any(p in col_lower for p in numeric_patterns):
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Ana rÃ¼zgar kolonlarÄ±nÄ± yeniden adlandÄ±r
            for col in df.columns:
                col_lower = col.lower()
                if col_lower == 'forecast':
                    df = df.rename(columns={col: 'wind_forecast'})
                    logger.info(f"  '{col}' -> 'wind_forecast' olarak yeniden adlandÄ±rÄ±ldÄ±")
                elif col_lower == 'generation':
                    df = df.rename(columns={col: 'wind_generation'})
                    logger.info(f"  '{col}' -> 'wind_generation' olarak yeniden adlandÄ±rÄ±ldÄ±")
            
            # Gereksiz quarter kolonlarÄ±nÄ± kaldÄ±r (15 dakikalÄ±k detay gereksiz)
            quarter_cols = [c for c in df.columns if 'quarter' in c.lower()]
            if quarter_cols:
                df = df.drop(columns=quarter_cols, errors='ignore')
                logger.info(f"  Quarter kolonlarÄ± kaldÄ±rÄ±ldÄ±: {quarter_cols}")
            
            # Saatlik gruplama yap (15 dakikalÄ±k veri varsa)
            if not isinstance(df.index, pd.DatetimeIndex):
                return df
            
            # EÄŸer veri 15 dakikalÄ±k ise saatliÄŸe Ã§evir
            time_diff = df.index.to_series().diff().median()
            if time_diff and time_diff < pd.Timedelta(hours=1):
                logger.info(f"  15 dakikalÄ±k veri saatliÄŸe dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
                # Sadece sayÄ±sal kolonlarÄ± grupla
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_cols:
                    df = df[numeric_cols].resample('h').mean()
                    logger.info(f"  Saatlik veri: {len(df)} satÄ±r")
        
        return df
    
    def fetch_realtime_generation(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        GerÃ§ek zamanlÄ± Ã¼retim verilerini Ã§eker (kaynak bazlÄ±).
        
        NOT: Bu endpoint bazÄ± hesaplarda aktif olmayabilir.
        Hata alÄ±nÄ±rsa boÅŸ DataFrame dÃ¶ner.
        
        Returns:
            DataFrame: Kaynak bazlÄ± Ã¼retim verileri
        """
        logger.info("ğŸ“Š GerÃ§ek zamanlÄ± Ã¼retim Ã§ekiliyor...")
        
        # FarklÄ± endpoint isimleri dene
        possible_calls = ["rt-gen", "generation", "real-time-generation"]
        
        for call_name in possible_calls:
            try:
                df = self._fetch_data(call_name, start_date, end_date)
                if not df.empty:
                    df = self._standardize_datetime(df)
                    return df
            except Exception as e:
                logger.warning(f"  {call_name} Ã§alÄ±ÅŸmadÄ±: {e}")
                continue
        
        logger.warning("  âš  GerÃ§ek zamanlÄ± Ã¼retim verisi Ã§ekilemedi (bu normal olabilir)")
        return pd.DataFrame()
    
    def fetch_dpp(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        KGÃœP (KesinleÅŸmiÅŸ GÃ¼nlÃ¼k Ãœretim PlanÄ±) verilerini Ã§eker.
        
        Returns:
            DataFrame: Organizasyon bazlÄ± Ã¼retim planlarÄ±
        """
        logger.info("ğŸ“Š KGÃœP verileri Ã§ekiliyor...")
        df = self._fetch_data("dpp-org", start_date, end_date)
        
        if not df.empty:
            df = self._standardize_datetime(df)
        
        return df
    
    def _standardize_datetime(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        DataFrame'deki tarih/saat kolonlarÄ±nÄ± standartlaÅŸtÄ±rÄ±r.
        
        EPÄ°AÅ API'sinden gelen farklÄ± kolon formatlarÄ±nÄ± handle eder.
        
        Returns:
            DataFrame: datetime indeksli standart format
        """
        if df.empty:
            return df
            
        df = df.copy()
        
        # Debug: Gelen kolonlarÄ± logla
        logger.debug(f"  Gelen kolonlar: {list(df.columns)}")
        
        # OlasÄ± tarih kolon isimleri (kÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅŸtÃ¼rerek arayacaÄŸÄ±z)
        date_patterns = ['date', 'tarih', 'period', 'gun']
        hour_patterns = ['hour', 'saat', 'period']
        datetime_patterns = ['datetime', 'timestamp', 'time']
        
        # Kolon isimlerini kÃ¼Ã§Ã¼k harfe Ã§evir
        col_mapping = {c: c.lower() for c in df.columns}
        
        # datetime kolonu varsa direkt kullan
        for col in df.columns:
            col_lower = col.lower()
            if any(p in col_lower for p in datetime_patterns) and 'date' in col_lower:
                try:
                    df['datetime'] = pd.to_datetime(df[col])
                    df = df.set_index('datetime').sort_index()
                    logger.debug(f"  datetime index oluÅŸturuldu: {col}")
                    return df
                except Exception as e:
                    logger.debug(f"  {col} datetime'a Ã§evrilemedi: {e}")
        
        # date ve hour ayrÄ± ayrÄ± ara
        date_col = None
        hour_col = None
        
        for col in df.columns:
            col_lower = col.lower()
            if date_col is None and any(p in col_lower for p in date_patterns):
                date_col = col
            if hour_col is None and any(p in col_lower for p in hour_patterns) and 'date' not in col_lower:
                hour_col = col
        
        # date ve hour bulunduysa birleÅŸtir
        if date_col:
            try:
                df['datetime'] = pd.to_datetime(df[date_col])
                
                if hour_col:
                    # Saat kolonunu ekle
                    df['datetime'] = df['datetime'] + pd.to_timedelta(df[hour_col], unit='h')
                
                df = df.set_index('datetime').sort_index()
                
                # Gereksiz kolonlarÄ± temizle
                cols_to_drop = [date_col]
                if hour_col:
                    cols_to_drop.append(hour_col)
                df = df.drop(columns=cols_to_drop, errors='ignore')
                
                logger.debug(f"  datetime index oluÅŸturuldu: {date_col} + {hour_col}")
                return df
                
            except Exception as e:
                logger.warning(f"  datetime oluÅŸturma hatasÄ±: {e}")
        
        # HiÃ§bir ÅŸey bulunamadÄ±ysa, ilk kolonu index yap
        if not isinstance(df.index, pd.DatetimeIndex):
            try:
                # Ä°lk kolonun tarih olup olmadÄ±ÄŸÄ±nÄ± kontrol et
                first_col = df.columns[0]
                df['datetime'] = pd.to_datetime(df[first_col])
                df = df.set_index('datetime').sort_index()
                logger.debug(f"  Ä°lk kolon datetime olarak kullanÄ±ldÄ±: {first_col}")
            except:
                logger.warning("  datetime index oluÅŸturulamadÄ±, ham veri dÃ¶ndÃ¼rÃ¼lÃ¼yor")
        
        return df
    
    def fetch_all(
        self, 
        start_date: Optional[str] = None, 
        end_date: Optional[str] = None
    ) -> Dict[str, pd.DataFrame]:
        """
        TÃ¼m gerekli verileri Ã§eker.
        
        Args:
            start_date: BaÅŸlangÄ±Ã§ tarihi (None ise config'den alÄ±nÄ±r)
            end_date: BitiÅŸ tarihi (None ise bugÃ¼n)
            
        Returns:
            Dict: Veri tÃ¼rÃ¼ -> DataFrame
        """
        start_date = start_date or self.settings.data.start_date
        end_date = end_date or datetime.now().strftime("%Y-%m-%d")
        
        logger.info(f"\n{'='*50}")
        logger.info(f"VERÄ° Ã‡EKME BAÅLADI: {start_date} â†’ {end_date}")
        logger.info(f"{'='*50}\n")
        
        data = {}
        
        # 1. PTF (Ana hedef deÄŸiÅŸken)
        data['ptf'] = self.fetch_ptf(start_date, end_date)
        
        # 2. SMF (Sistem dengesizlik sinyali)
        data['smf'] = self.fetch_smf(start_date, end_date)
        
        # 3. YÃ¼k Tahmini
        data['load'] = self.fetch_load_plan(start_date, end_date)
        
        # 4. RÃ¼zgar Tahmini
        data['wind'] = self.fetch_wind_forecast(start_date, end_date)
        
        # 5. GerÃ§ek ZamanlÄ± Ãœretim
        data['generation'] = self.fetch_realtime_generation(start_date, end_date)
        
        logger.info(f"\n{'='*50}")
        logger.info("VERÄ° Ã‡EKME TAMAMLANDI")
        for name, df in data.items():
            if not df.empty:
                logger.info(f"  {name}: {len(df)} satÄ±r")
        logger.info(f"{'='*50}\n")
        
        return data


class DataMerger:
    """
    FarklÄ± veri kaynaklarÄ±nÄ± birleÅŸtiren sÄ±nÄ±f.
    PTF tahminlemesi iÃ§in tek bir DataFrame oluÅŸturur.
    """
    
    @staticmethod
    def merge_datasets(data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        TÃ¼m veri setlerini birleÅŸtirir.
        
        Args:
            data: fetch_all() Ã§Ä±ktÄ±sÄ±
            
        Returns:
            DataFrame: BirleÅŸtirilmiÅŸ veri seti
        """
        logger.info("ğŸ”— Veri setleri birleÅŸtiriliyor...")
        
        # PTF ana tablo olacak
        if 'ptf' not in data or data['ptf'].empty:
            # Alternatif: diÄŸer veri setlerinden birini kullan
            for key in ['smf', 'load', 'wind']:
                if key in data and not data[key].empty:
                    logger.warning(f"PTF verisi bulunamadÄ±, {key} ana tablo olarak kullanÄ±lÄ±yor")
                    data['ptf'] = data[key].copy()
                    break
            else:
                raise ValueError("PTF verisi bulunamadÄ± ve alternatif veri de yok!")
        
        merged = data['ptf'].copy()
        
        # Index'in datetime olduÄŸundan emin ol
        if not isinstance(merged.index, pd.DatetimeIndex):
            # datetime kolonu ara
            dt_cols = [c for c in merged.columns if 'date' in c.lower() or 'time' in c.lower()]
            if dt_cols:
                merged['datetime'] = pd.to_datetime(merged[dt_cols[0]])
                merged = merged.set_index('datetime')
        
        # DiÄŸer verileri birleÅŸtir
        for name, df in data.items():
            if name == 'ptf' or df.empty:
                continue
            
            try:
                # Index'in uyumlu olduÄŸundan emin ol
                if not isinstance(df.index, pd.DatetimeIndex):
                    dt_cols = [c for c in df.columns if 'date' in c.lower() or 'time' in c.lower()]
                    if dt_cols:
                        df['datetime'] = pd.to_datetime(df[dt_cols[0]])
                        df = df.set_index('datetime')
                
                # Index bazlÄ± merge (datetime index)
                merged = merged.join(df, how='left', rsuffix=f'_{name}')
                logger.info(f"  âœ“ {name} birleÅŸtirildi ({len(df)} satÄ±r)")
            except Exception as e:
                logger.warning(f"  âš  {name} birleÅŸtirilemedi: {e}")
        
        # Duplicate kolonlarÄ± temizle
        merged = merged.loc[:, ~merged.columns.duplicated()]
        
        # Index'i sÄ±rala
        merged = merged.sort_index()
        
        logger.info(f"  â†’ Final shape: {merged.shape}")
        
        return merged
    
    @staticmethod
    def validate_data(df: pd.DataFrame) -> Tuple[bool, List[str]]:
        """
        Veri kalitesini kontrol eder.
        
        Returns:
            Tuple[bool, List[str]]: (GeÃ§erli mi?, UyarÄ±lar listesi)
        """
        warnings = []
        
        # 1. Eksik veri kontrolÃ¼
        missing_pct = (df.isnull().sum() / len(df) * 100).round(2)
        high_missing = missing_pct[missing_pct > 5]
        if not high_missing.empty:
            warnings.append(f"YÃ¼ksek eksik veri: {high_missing.to_dict()}")
        
        # 2. PTF deÄŸer aralÄ±ÄŸÄ± kontrolÃ¼
        if 'ptf' in df.columns:
            ptf_min, ptf_max = df['ptf'].min(), df['ptf'].max()
            if ptf_min < 0:
                warnings.append(f"Negatif PTF deÄŸeri: {ptf_min}")
            if ptf_max > 10000:  # Anormal yÃ¼ksek
                warnings.append(f"Anormal yÃ¼ksek PTF: {ptf_max}")
        
        # 3. Zaman sÃ¼rekliliÄŸi kontrolÃ¼
        if isinstance(df.index, pd.DatetimeIndex):
            time_diff = df.index.to_series().diff()
            gaps = time_diff[time_diff > pd.Timedelta(hours=1)]
            if not gaps.empty:
                warnings.append(f"Zaman boÅŸluklarÄ±: {len(gaps)} adet")
        
        is_valid = len(warnings) == 0
        return is_valid, warnings


# Test ve kullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    print("\n" + "="*60)
    print("EPÄ°AÅ VERÄ° Ã‡EKME TESTÄ°")
    print("="*60 + "\n")
    
    try:
        # Settings yÃ¼kle
        settings = get_settings()
        print(f"âœ“ Config yÃ¼klendi")
        print(f"  Username: {settings.epias.username[:3]}***")
        print(f"  Tarih aralÄ±ÄŸÄ±: {settings.data.start_date} - bugÃ¼n")
        
        # Fetcher oluÅŸtur
        fetcher = EPIASDataFetcher(settings)
        
        # Test iÃ§in kÄ±sa bir aralÄ±k Ã§ek
        test_start = "2024-01-01"
        test_end = "2024-01-07"
        
        print(f"\nğŸ“¥ Test verisi Ã§ekiliyor: {test_start} - {test_end}")
        
        data = fetcher.fetch_all(test_start, test_end)
        
        # Verileri birleÅŸtir
        merger = DataMerger()
        merged_df = merger.merge_datasets(data)
        
        # Validasyon
        is_valid, warnings = merger.validate_data(merged_df)
        
        print("\n" + "="*60)
        print("SONUÃ‡")
        print("="*60)
        print(f"Veri geÃ§erliliÄŸi: {'âœ“ GeÃ§erli' if is_valid else 'âš  UyarÄ±lar var'}")
        for w in warnings:
            print(f"  - {w}")
        print(f"\nFinal DataFrame:")
        print(merged_df.head())
        
    except Exception as e:
        print(f"\nâœ— Hata: {e}")
        print("\nLÃ¼tfen config/config.yaml dosyasÄ±nÄ± kontrol edin.")
